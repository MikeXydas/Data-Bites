{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Airline Review Analysis - In Progress\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-30T20:47:16.136134Z",
     "start_time": "2020-04-30T20:47:14.583064Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mikexydas/pythonEnvs/thesisEnv/lib/python3.6/site-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.metrics.scorer module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.metrics. Anything that cannot be imported from sklearn.metrics is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/home/mikexydas/pythonEnvs/thesisEnv/lib/python3.6/site-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.feature_selection.base module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.feature_selection. Anything that cannot be imported from sklearn.feature_selection is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "import random\n",
    "import unicodedata\n",
    "import re\n",
    "from email.message import EmailMessage\n",
    "import smtplib\n",
    "import ssl\n",
    "\n",
    "import nltk\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from wordcloud import WordCloud\n",
    "import eli5\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Gathering\n",
    "\n",
    "Initially, I thought of scrapping the [skytrax](vhttps://www.airlinequality.com/) website, which seemed pretty easy. Before doing that however, a quick search showed me that someone has already done that and the [data uploaded on Kaggle](https://www.kaggle.com/efehandanisman/skytrax-airline-reviews) is fairly recent. \n",
    "  \n",
    "Actually, the fact that it does not include reviews of the last months may be a good thing. As I was browsing skytrax I noticed many recent negative reviews of cancellations, new no-refund policies and lockdown measures, that were caused by Covid-19. Hopefully, this does not reflect the reality of air traveling and we avoid using it in our analysis.  \n",
    "  \n",
    "The data is in .xlsx (excel) form. I have avoided converting the file on a .csv so as everyone can download the data from the above Kaggle link and play with the notebook with out having to convert. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We drop the aircraft column since it has many NaN values and is not important for our analysis\n",
    "df = pd.read_excel('datasets/capstone_airline_reviews3.xlsx').drop(['aircraft'], axis=1)\n",
    "\n",
    "# Many fields will remain empty, but depending on our use case we will accordingly deal with them\n",
    "df = df.dropna(how='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a first read for our dataset and we have a lot of cleaning up to do. Depending on what data we need for our analysis we must deal with the remaining `NaN` values.  \n",
    "Also on the `customer_review` field we can see some unicode characters that must be dealt with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Most Important Rating\n",
    "\n",
    "Each review has a number of ratings:\n",
    "* Food & Beverages\n",
    "* Inflight Entertainment\n",
    "* Seat Comfort\n",
    "* Value For Money\n",
    "* Cabin Staff Service\n",
    "* Ground Service\n",
    "\n",
    "THe ratings have a value from 1 (being the worst) to 5 (being the best). I would like to find which one of the ratings is the most correlated with **if he would recommend the flight**.  \n",
    "  \n",
    "Why I consider the *recommend* value as the best reflection of passenger satisfaction?\n",
    "If a passenger recommends an airline then this means two things. Firstly, that he would promote to a friend/family the airline. Secondly, in future air traveling is probable that hew would again choose the same airline if possible. From a business perspective I believe this makes the *recommend* metric important.  \n",
    "\n",
    "### Missing Values\n",
    "\n",
    "As expected many of the reviews have missing values. The quick approach is to drop all the rows with missing values on the columns we need. We will do that and check how many rows we lose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the columns we need from the df\n",
    "rate_importance_df = df.iloc[:,-7:].copy()\n",
    "\n",
    "# We drop the rows that have a NaN value on the recommended column\n",
    "rate_importance_df = rate_importance_df[rate_importance_df['recommended'].notna()]\n",
    "\n",
    "print(f\"Rows including NaNs: {len(rate_importance_df)}\")\n",
    "\n",
    "rate_importance_df = rate_importance_df.dropna()\n",
    "\n",
    "print(f\"Rows without NaNs: {len(rate_importance_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We lose about 40.000 rows but we still have ~25.000 rows. At least initially I feel that the remaining rows are enough and no other strategy is followed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation\n",
    "\n",
    "We will now calculate the correlation of each rating with if the *recommendation* value.  \n",
    "  \n",
    "Our results will not have a simple interpretation, meaning we must take into account that [**correlation does not imply causation**](https://www.tylervigen.com/spurious-correlations).  \n",
    "  \n",
    "**Hypothetical scenario:**  \n",
    "Let's say we found a great correlation (r >0.9) between recommend and inflight entertainment. One might suggest we should make our flights more entertaining, so we install TVs, music, etc. but after some months we do not have the expected increase on ratings.  \n",
    "With further analysis we discover that the duration of the flight is highly correlated with the inflight entertainment and consequently with the recommend value.  \n",
    "This means that long haul flights make the passengers more easily bored and tired and we cannot actually do much about this specific issue since [airplanes will not get much faster](https://aviation.stackexchange.com/a/31386)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the yes/no values on recommend column to 1/0\n",
    "rate_importance_df.recommended = rate_importance_df.recommended.map(dict(yes=1, no=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our case we have a continuous value for the ratings and a binary value (yes/no) for the recommend value. So, we will use the [point-biserial correlation coefficient](https://en.wikipedia.org/wiki/Point-biserial_correlation_coefficient) which is a [special case of Pearson correlation](https://stats.stackexchange.com/a/105553).  \n",
    "  \n",
    "Firstly, I will plot some boxplots which will be a pretty good indication if there is any correlation, before quantifying it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(18, 13))\n",
    "plot_index = 1\n",
    "\n",
    "# Iterate on all the ratings\n",
    "for which_rating in rate_importance_df.columns[:-1]:\n",
    "    ax = fig.add_subplot(2, 3, plot_index)\n",
    "    yes_ratings = np.array(rate_importance_df.loc[rate_importance_df['recommended'] == 1][which_rating])\n",
    "    no_ratings = np.array(rate_importance_df.loc[rate_importance_df['recommended'] == 0][which_rating])\n",
    "\n",
    "    ax = sns.boxplot(data=[no_ratings, yes_ratings])\n",
    "    plt.xticks(range(0, 2), [\"no\", \"yes\"])\n",
    "    plt.title(which_rating)\n",
    "    plt.ylabel(\"Rating\")\n",
    "    plt.xlabel(\"Recommended\")\n",
    "    plot_index += 1\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the above plots we can see that all the ratings are correlated. A low rating on any of these, means a \"no\" on recommendation and vice versa.  \n",
    "An observation is, that when we are rating **we are biased from our overall experience**. That means if for example the ground_service is bad this might affect our whole view of the flight negatively and be biased when rating the cabin_service etc.  \n",
    "This can be shown by calculating the correlations between ratings. This shows **correlation \"chains\"** which do not allow us to be able to distinguish which one is the most important rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's calculate the correlation\n",
    "for which_rating in rate_importance_df.columns[:-1]:\n",
    "    r, p = stats.pointbiserialr(rate_importance_df.recommended, rate_importance_df[which_rating])\n",
    "    print(f\"{which_rating},\\tr = {np.round(r, decimals=3)} with p-value={p}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see most of them have the same levels of r-coefficients with entertainment being a bit lower and value for money a bit higher (which is expected since value for money can be considered as a summarizing rating of all the other ratings).  \n",
    "  \n",
    "**So by simply looking at the ratings I believe we cannot have a decisive answer on what the airlines should improve.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review Text Processing\n",
    "\n",
    "Since the above analysis does not have a clear results we will now try to have insights from the text of the reviews. This sounds more helpful, because although the ratings are biased from the overall experience the text should be focused on more specific events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Again we will not use all the columns of the df\n",
    "reviews_df = df[['customer_review', 'recommended']].copy()\n",
    "reviews_df = reviews_df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning the Reviews\n",
    "\n",
    "The reviews as expected contain a lot of non ascii characters. We will review how many of them there are and decide if we should delete them or encode them on ascii."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find how many reviews have non-ascii characters\n",
    "mask_nonAscii = reviews_df['customer_review'].str.len()\\\n",
    "                 .ne(reviews_df['customer_review'].str.encode('ascii',errors = 'ignore').str.len())\n",
    "print(f'Reviews with non ascii characters: {sum(mask_nonAscii)} out of {len(reviews_df)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since 28238 is a big number of reviews we will see if we can avoid deleting them. For this reason I will print some random reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    rev = reviews_df[mask_nonAscii].iloc[random.sample(range(0, len(reviews_df[mask_nonAscii])), 1)[0]].customer_review\n",
    "    print(f'{rev}\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After some prints of the reviews that have non-ascii characters we notice some things:\n",
    "* The non-ascii characters are the first three \"âœ\"\n",
    "* Most of the reviews have at the start \"Trip Verified |\" and then the trip departing and arriving location  \n",
    "  \n",
    "We first deal with two above and then count again the non-ascii reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split on the first '.' and keep the second part, CARE: This cannot be executed two consecutive times\n",
    "reviews_df['customer_review'] = reviews_df['customer_review'] .map(\n",
    "    lambda x: x.split('.', 1)[1] if len(x.split('.', 1)) > 1 \n",
    "    else None\n",
    ")\n",
    "\n",
    "\n",
    "# Drop 26 rows taht ended up not having anythong after their first '.'\n",
    "reviews_df = reviews_df.dropna()\n",
    "\n",
    "# Let's again count the non-ascii reviews\n",
    "mask_nonAscii = reviews_df['customer_review'].str.len()\\\n",
    "                 .ne(reviews_df['customer_review'].str.encode('ascii',errors = 'ignore').str.len())\n",
    "print(f'Reviews with non ascii characters: {sum(mask_nonAscii)} out of {len(reviews_df)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    rev = reviews_df[mask_nonAscii].iloc[random.sample(range(0, len(reviews_df[mask_nonAscii])), 1)[0]].customer_review\n",
    "    print(f'{rev}\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After again printing some non-ascii reviews we notice that in most cases it is caused by **the apostrophe of [contractions](https://dictionary.cambridge.org/grammar/british-grammar/contractions) (didn't, haven't, etc) not being the ASCII apostrophe**. Since these words do not have a significant meaning that differentiates a positive from a negative review I believe we are allowed to just encode the strings on ASCII."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decode all the non-ascii characters from the reviews\n",
    "reviews_df['customer_review'] = reviews_df['customer_review'].apply(lambda x: x.encode('ascii', 'ignore').\\\n",
    "                                                              strip())\n",
    "\n",
    "# Go from bytes object to strings\n",
    "reviews_df['customer_review'] = reviews_df['customer_review'].str.decode(\"utf-8\")\n",
    "\n",
    "# Final check that there no non-ascii characters in all the reviews\n",
    "mask_nonAscii = reviews_df['customer_review'].str.len()\\\n",
    "                 .ne(reviews_df['customer_review'].str.encode('ascii',errors = 'ignore').str.len())\n",
    "print(f'Reviews with non ascii characters: {sum(mask_nonAscii)} out of {len(reviews_df)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Normalization\n",
    "\n",
    "Now we reach the point that we want to see which words seem to be mostly used by the travelers in their reviews. Of course, words like \"and\", \"have\", etc will have a big frequency. Using a stopwords dictionary we will remove all of these words that do not offer us any information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_review(review):\n",
    "    \"\"\"\n",
    "    We will lower case all the words, remove punctuation, remove stopwords and lemmatize the verbs\n",
    "    \n",
    "    Args:\n",
    "        review (str): The original review (in ascii)\n",
    "        \n",
    "    Returns:\n",
    "        str: The normalized review\n",
    "    \"\"\"\n",
    "    words = nltk.word_tokenize(review)\n",
    "    \n",
    "    # Lower words\n",
    "    lower_words = [word.lower() for word in words]\n",
    "    \n",
    "    # Remove punctuation\n",
    "    words_no_punctuation = []\n",
    "    for word in lower_words:\n",
    "        new_word = re.sub(r'[^\\w\\s]', '', word)\n",
    "        if new_word != '':\n",
    "            words_no_punctuation.append(new_word)\n",
    "    \n",
    "    # Remove stopwords\n",
    "    words_no_stopwords = []\n",
    "    for word in words_no_punctuation:\n",
    "        if word not in stopwords.words('english'):\n",
    "            words_no_stopwords.append(word)\n",
    "    \n",
    "    # Lemmatize the verbs (eg ran -> run)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word, pos='v') for word in words_no_punctuation]\n",
    "            \n",
    "    return \" \".join(lemmatized_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Warning**: The below cell takes **16 minutes** in my laptop. In case you want to run the cells below you could first slice the `reviews_df` and select some of the rows, like:\n",
    "```python\n",
    "# Random sample of 5000 rows\n",
    "reviews_df = reviews_df.sample(n=5000)\n",
    "```\n",
    "The normalized reviews were cached on my laptop so as to not have to execute this cell again, but the size of the file was way too big for the remote repo.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-30T20:47:33.436357Z",
     "start_time": "2020-04-30T20:47:32.923469Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_review</th>\n",
       "      <th>recommended</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>first time id fly tk i find them very good in ...</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>we make our check in in the airport they take ...</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>i fly with this company several time in the pa...</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>turkish airlines have consistently maintain it...</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>never book turkish airlines if you be travel t...</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131886</th>\n",
       "      <td>manual checkin swift plan old 737 but clean an...</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131888</th>\n",
       "      <td>check in be very fast much quicker than my exp...</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131890</th>\n",
       "      <td>have fly with uia so many time i be expect tha...</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131892</th>\n",
       "      <td>mix experience have be yell at once try to cor...</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131894</th>\n",
       "      <td>although it be a relatively short flight a goo...</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>64414 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          customer_review recommended\n",
       "1       first time id fly tk i find them very good in ...         yes\n",
       "3       we make our check in in the airport they take ...          no\n",
       "5       i fly with this company several time in the pa...          no\n",
       "7       turkish airlines have consistently maintain it...         yes\n",
       "9       never book turkish airlines if you be travel t...          no\n",
       "...                                                   ...         ...\n",
       "131886  manual checkin swift plan old 737 but clean an...          no\n",
       "131888  check in be very fast much quicker than my exp...          no\n",
       "131890  have fly with uia so many time i be expect tha...          no\n",
       "131892  mix experience have be yell at once try to cor...          no\n",
       "131894  although it be a relatively short flight a goo...          no\n",
       "\n",
       "[64414 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tqdm.pandas()\n",
    "\n",
    "# Normalize all the reviews\n",
    "# reviews_df['customer_review'] = reviews_df['customer_review'].progress_apply(lambda x : normalize_review(x))\n",
    "\n",
    "# This file is not included on the remote repo\n",
    "reviews_df = pd.read_csv('datasets/cache/normalized_reviews.csv', index_col=0)\n",
    "\n",
    "display(reviews_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wordclouds\n",
    "\n",
    "One quick way to visualize words with high frequencies is to use wordclouds. We will create one for the `recommended=\"yes\"` and one for the `recommended=\"no\"`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recomended_yes_text = \" \".join(review for review \n",
    "                               in reviews_df.loc[reviews_df['recommended'] == \"yes\"].customer_review)\n",
    "\n",
    "recomended_no_text = \" \".join(review for review \n",
    "                               in reviews_df.loc[reviews_df['recommended'] == \"no\"].customer_review)\n",
    "\n",
    "wordcloud_yes = WordCloud(background_color=\"white\").generate(recomended_yes_text)\n",
    "wordcloud_no = WordCloud(background_color=\"white\").generate(recomended_no_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(18, 10))\n",
    "\n",
    "ax = fig.add_subplot(2, 1, 1)\n",
    "plt.title(\"Recommended\")\n",
    "plt.imshow(wordcloud_yes, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "\n",
    "\n",
    "ax = fig.add_subplot(2, 1, 2)\n",
    "plt.title(\"Not Recommended\")\n",
    "plt.imshow(wordcloud_no, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observing the not recommended wordcloud we can easily see some expected words like:\n",
    "* flight delay\n",
    "* customer sercive\n",
    "* connect flight\n",
    "  \n",
    "However, we must \"quantify\" the observations of this wordcloud using other techniques.  \n",
    "  \n",
    "Before doing that I was curious to find out why word \"one\" had such a high frequency on both recommended and not recommended reviews.  \n",
    "The cell below prints reviews containing \"one\" and we can see that it is just a commonly used word that has no significant meaning in the traveling reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "reviews_df_no = reviews_df.loc[reviews_df['recommended'] == \"no\"].copy()\n",
    "contains_one = reviews_df_no[reviews_df_no['customer_review'].str.contains(\"one\")]\n",
    "\n",
    "# Print 2 random reviews containing the word \"one\"\n",
    "for i in range(2):\n",
    "    print(f'{contains_passenger.iloc[random.sample(range(0, len(contains_passenger)), 1)[0]].customer_review}\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review Text Classification\n",
    "\n",
    "One other way of dealing with our problem is to train a classifier to distinguish recommended and NOT recommended reviews. However, we will not use the model to predict future reviews (this is done by the users submitting their review already).  \n",
    "We will study the weights of the classifiers in order to find the most important features (words).  \n",
    "  \n",
    "*We will not focus on ML techniques like hyperparameter tuning and cross-validation since our main focus is not to create an actual predictive model*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-30T21:45:51.308721Z",
     "start_time": "2020-04-30T21:45:51.298929Z"
    }
   },
   "outputs": [],
   "source": [
    "# Since I let the notebook run, I have set up a throw away account that notifies me when it has ended\n",
    "# I will delete it as soon as I have ended with the notebook.\n",
    "# Please do not use it since there are security risks\n",
    "def notify_me(address, content, password):\n",
    "    port = 465  # For SSL\n",
    "    smtp_server = \"smtp.gmail.com\"\n",
    "    sender_email = \"jup.notify@gmail.com\"  # A throwaway email I have created for Jupyter notifications\n",
    "    receiver_email = address\n",
    "    message = f\"\"\"{content}\"\"\"\n",
    "\n",
    "    context = ssl.create_default_context()\n",
    "    with smtplib.SMTP_SSL(smtp_server, port, context=context) as server:\n",
    "        server.login(sender_email, password)\n",
    "        server.sendmail(sender_email, receiver_email, message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-30T21:58:48.116129Z",
     "start_time": "2020-04-30T21:58:48.081662Z"
    }
   },
   "outputs": [],
   "source": [
    "# Shuffle the dataset so as our split of train - test is more robust\n",
    "reviews_df_used = reviews_df.sample(frac=0.3)    # Due to the dataset bein a bit big for my laptop I will use all the reviews\n",
    "reviews_df_used = reviews_df_used.dropna()\n",
    "\n",
    "# Split the train and test sets\n",
    "train_X = reviews_df_used.iloc[:round(0.8 * len(reviews_df_used)), 0]\n",
    "train_Y = reviews_df_used.iloc[round(0.8 * len(reviews_df_used)):, 0]\n",
    "\n",
    "test_X = reviews_df_used.iloc[:round(0.8 * len(reviews_df_used)), 1]\n",
    "test_Y = reviews_df_used.iloc[round(0.8 * len(reviews_df_used)):, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-04-30T21:58:48.711Z"
    }
   },
   "outputs": [],
   "source": [
    "vec = CountVectorizer()\n",
    "clf = SVC(kernel='linear')    # We have to use linear kernel so as the results are interpretable by ELI5 package\n",
    "pipe = make_pipeline(vec, clf)\n",
    "pipe.fit(train_X, test_X);\n",
    "\n",
    "notify_me(\"mikexydas@gmail.com\", \"Finished\", \"SVM has finished\", password)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although we do not care about the predictive ability of our model we must evaluate it in order to make sure that the weights have useful meaning to us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-30T21:58:30.325398Z",
     "start_time": "2020-04-30T21:58:28.728126Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         yes       0.89      0.88      0.88       696\n",
      "          no       0.86      0.87      0.86       591\n",
      "\n",
      "    accuracy                           0.87      1287\n",
      "   macro avg       0.87      0.87      0.87      1287\n",
      "weighted avg       0.87      0.87      0.87      1287\n",
      "\n",
      "accuracy: 0.873\n"
     ]
    }
   ],
   "source": [
    "y_pred = pipe.predict(train_Y)\n",
    "report = metrics.classification_report(test_Y, y_pred, \n",
    "    target_names=['yes', 'no'])\n",
    "print(report)\n",
    "print(\"accuracy: {:0.3f}\".format(metrics.accuracy_score(test_Y, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see we have some acceptable predicting power with almost no tuning. This allows us to believe that the weights of the words have an actual differentiating meaning between \"yes\" and \"no\" reviews.  \n",
    "  \n",
    "In order to get which words were more important we will use the ML debugging package called [ELI5](https://github.com/TeamHG-Memex/eli5). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eli5.show_weights(clf, vec=vec, top=10, \n",
    "                  target_names=['yes', 'no'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit ('thesisEnv': virtualenv)",
   "language": "python",
   "name": "python36964bitthesisenvvirtualenv849bc23effdd4f5cbcfcfcad50606969"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "notify_time": "5",
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
